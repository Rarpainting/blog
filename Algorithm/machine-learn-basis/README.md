# 人工智能基础课

## 数学基础 -- 概率论

频率学派:

- 频率学派的概率: 可独立重复的随机试验中单个结果出现频率的 **极限**; 即试验的结果只包含有限个给本事件, 且每个基本事件发生的可能性 **相同**
- 频率学派认为先验分布是固定的, 模型参数依靠 **最大似然估计** 计算
- 假设, 是客观存在且不会改变的, 存在固定的先验分布

贝叶斯学派:

- 贝叶斯学派的概率: 随机事件的可信程度
- 贝叶斯学派认为先验分布是随机的, 模型参数依靠 **后验概率最大化** 计算
- 固定的先验分布是不存在的, 参数本身也是随机数

## 数理统计

- 数理统计, 通过可观察的样本反推断总体的性质
- 推断的工具是统计量, 统计量是样本的函数, 是个随机变量
- 参数估计通过随机抽取的样本来估计总体分布的未知参数, 包括点估计和区间估计
- 假设检验通过随机抽取的样本接受或拒绝关于总体的某个判断, 常用于估计机器学习模型的泛化错误率

## 信息论

### 信息熵

一个系统内在的混乱程度:

![熵](img/equation-message-1.svg)

$S(\Gamma)=-\sum_{x\in \Gamma} p(x)\ln p(x)$

其中:
- $\Gamma$ 是相空间
- $x$ 是相空间中的态

意义: 对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释, 并在世界的不确定性和信息的可测量性之间搭建起一座桥梁

### 信息量

如果事件 $A$ 发生的概率为 $P(A)$, 则这个事件的信息量定义为:

$H(A) = - \log_2 P(A)$

### 互信息/信息增益

$I(X; Y) = H(Y) - H(Y|X)$

$I(X; Y)$ : X 给 Y 的信息增益 -- 特征 X 对 训练集 Y 的区分度
$H(Y)$ : 未给定任何特征时, 对训练集进行分类的不确定性
$H(Y|X)$ : 使用特征 X 对训练集 Y 进行分类的不确定性

信息增益比:

$g(X, Y) = I(X; Y)/H(Y)$

### KL 散度

描述两个概率分布 P 和 Q 之间的差异的一种方法:

$D_{KL}(P||Q) = \sum_{i = 1}^n p(x_i) \log_2 \frac{p(x_i)}{q(x_i)}$

KL 散度是对额外信息量的衡量

#### 非负性

KL 散度 >=0 , 当且仅当两个分布相同时, 取等

#### 非对称性

$D_{KL}(P||Q) \ne D_{KL}(Q||P)$
即, 通过 $P(X)$ 去近似 $Q(X)$ 和 通过 $Q(X)$ 去近似 $P(X)$ 得到的偏差, 并不相同

#### 最大熵原理

**对于一个未知的概率分布, 最坏的情况就是 该概率 以 等可能性 取到每个可能性的取值**

- 此时随机变量的随机程度最高, 预测也是最困难的
- 最大熵原理是为了在推断未知分布时不引入多余的约束和假设, 预测的 **风险** 最小, 同时得到最不确定的结果

#### 最大熵模型



## 凸集/凸函数/凸优化

凸集具有的几何性质是凸集中的任意两点都是 **无障碍可见的**
